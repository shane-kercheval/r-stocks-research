---
title: "Stocks Research"
author: "Shane Kercheval"
date: "Jan 21, 2017"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 2
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
source('./r_stocks_helpers.R', chdir=TRUE)
source('../r-tools/general/correlations.R', chdir=TRUE)
options(scipen=999) # non-scientific notation
options(width=180)
library(tidyverse)
library(knitr)
library(modelr)
library(mlbench)
library(caret)
library(stringr)
library(scales)
library(fpc)
library(RColorBrewer)
library(reshape2)

opts_chunk$set(out.width='750px', dpi=200)

cluster_heatmap <- function(results_df, start_stop=2)
{
	heatmap_breaks <- c(-Inf, seq(from = (-1 * start_stop), to = start_stop, by = (start_stop/5)), Inf)
	heatmap_colors <- c(rev(brewer.pal(n = 9,name = 'Blues')[2:7]), brewer.pal(n = 9,name = 'Reds')[2:7])

	results_df$cluster_name <- 1:nrow(results_df)
	results_df_melted <- melt(results_df, id.vars = 'cluster_name', measure.vars = colnames(results_df[-c(ncol(results_df))]))
	results_df_melted$means <- cut(results_df_melted$value, breaks = heatmap_breaks)

	heatmap <- ggplot(data = results_df_melted, aes(x = factor(variable, levels = rev(unique(variable))), y = cluster_name)) +
		geom_tile(aes(fill = means), colour = "white") +
		scale_fill_manual(values = heatmap_colors, drop = FALSE) + # drop=FALSE so that scale shows every value, even if not in dataset
		coord_flip()

	return (heatmap)
}

pull_data <- FALSE
if(pull_data)
{
    clean_cache()
}
```

# Preparing Stock Symbol Information

- stock symbols found at [Nasdaq.com](http://www.nasdaq.com/screening/company-list.aspx)
- stock information downloaded using [quantmod](https://cran.r-project.org/web/packages/quantmod/quantmod.pdf)

```{r stocks, message=FALSE, warning=FALSE, include=FALSE}
#######################################################################################################################################
# load and process stock symbols to download
#######################################################################################################################################
df_nasdaq_symbols <- read.csv('./data/companylist_nasdaq.csv', stringsAsFactors = FALSE) %>%
	filter(!grepl('_', symbol, fixed=TRUE), !grepl('.', symbol, fixed=TRUE), !grepl('^', symbol, fixed=TRUE))
df_nyse_symbols <- read.csv('./data/companylist_nyse.csv') %>%
	filter(!grepl('_', symbol, fixed=TRUE), !grepl('.', symbol, fixed=TRUE), !grepl('^', symbol, fixed=TRUE))
df_amex_symbols <- read.csv('./data/companylist_amex.csv') %>%
	filter(!grepl('_', symbol, fixed=TRUE), !grepl('.', symbol, fixed=TRUE), !grepl('^', symbol, fixed=TRUE))

# nasdaq requires extra processing because they don't have symbols for duplicate stock, but they have symbols like `BLVD`, `BLVDU`, `BLVDW` which all represent the same company, so we will always(?) want the shorter `BLVD`
df_nasdaq_symbols <- df_nasdaq_symbols %>%
						dplyr::select(symbol, name) %>%
						dplyr::group_by(name) %>%
						dplyr::summarise(symbol = min(symbol)) # group by company name and then grab the min (i.e. shortest in this case) symbol

all_symbols <- c(df_nasdaq_symbols$symbol, as.character(df_nyse_symbols$symbol), as.character(df_amex_symbols$symbol))
all_symbols <- sort(unique(str_trim(all_symbols))) # just in case we have duplciates

#######################################################################################################################################
# the index we will be comparing against (and trying to beat) is the iShares MSCI ACWI (ACWI) https://finance.yahoo.com/quote/ACWI?p=ACWI
#######################################################################################################################################
if(pull_data)
{
    acwi_closing <- get_prices('ACWI') #quarterly_mean = mean(acwi_closing$perc_change_90, na.rm = TRUE) # our mean is slightly thrown off since we add in additional numbers for weekends/holidays #yearly_return = (1+quarterly_mean)^4 - 1
    saveRDS(acwi_closing, file = 'acwi_closing.RDS')
}else
{
    acwi_closing <- readRDS(file = 'acwi_closing.RDS')
}
```

# Stock Symbol Datasets

These stock symbols will be used to download the financials using `quantmod`.

```{r show_symbols}
head(df_nasdaq_symbols)
head(df_nyse_symbols)
head(df_amex_symbols)
```

## All Country World Index (ACWI) Closing Prices

We will use the `All Country World Index` (`ACWI`) [closing prices](https://finance.yahoo.com/quote/ACWI?p=ACWI) to compare our stocks against (and trying to beat) with machine learning predictions. That is, we want to be able to select stocks that will outperform the market (i.e. `ACWI`) after 1 year from the release of the financial data.

According to [Investipedia](http://www.investopedia.com/terms/m/msci-acwi.asp), the ACWI is:

> a market capitalization weighted index designed to provide a broad measure of equity-market performance throughout the world. The MSCI ACWI is maintained by Morgan Stanley Capital International, and is comprised of stocks from both developed and emerging markets.

```{r show_acwi}
head(acwi_closing)
tail(acwi_closing)
```

# Downloading & Cleaning Stock Financial Data via `quantmod`

- downloads stock financials statements using `quantmod` library, saves them into partitions that contain up to 100 stocks per partition (some stocks will not be found)
- converts format of data returned by `quantmod` from various lists of financial statements to a single dataframes
- changes column names into valid and consistent values (e.g. removes punctuation)
- calculates various fields financial ratios and formats data so that rows (i.e. years) have that year's data as well as specific information for the following year in order to use those fields as target variables to be predicted (such as net profit margin, next year's stock prices, etc.)
	- removes stocks that are missing a certain threshold of information. 

```{r downloading_stocks, message=FALSE, warning=FALSE, include=FALSE}
#######################################################################################################################################
# download, filter, and save 100 stocks at a time to disk.
#######################################################################################################################################
partitions <- list(1:100, 101:200, 201:300, 301:400, 401:500, 501:600, 601:700, 701:800, 801:900, 901:1000, 1001:1100, 1101:1200, 1201:1300, 1301:1400, 1401:1500, 1501:1600, 1601:1700, 1701:1800, 1801:1900, 1901:2000, 2001:2100, 2101:2200, 2201:2300, 2301:2400, 2401:2500, 2501:2600, 2601:2700, 2701:2800, 2801:2900, 2901:3000, 3001:3100, 3101:3200, 3201:3300, 3301:3400, 3401:3500, 3501:3600, 3601:3700, 3701:3800, 3801:3900, 3901:4000, 4001:4100, 4101:4200, 4201:4300, 4301:4400, 4401:4500, 4501:4600, 4601:4700, 4701:4800, 4801:4900, 4901:5000, 5001:5100, 5101:5200, 5201:5300, 5301:5400, 5401:5500, 5501:5600, 5601:5700, 5701:5800, 5801: length(all_symbols))#5900, 5901:6000, 6001:6100, 6101:6200, 6201:6300, 6301:6400, 6401:6500, 6501:6600, 6601:6700, 6701:6800, 6801:6900, 6901:7000, 7001:7100, 7101:7200, 7201:7300, 7301:7400, 7401:7500, 7501:7600, 7601:7700, 7701:7800, 7801:7900, 7901:8000, 8001:8100, 8101:8200, 8201:8300, 8301:8400, 8401:8500, 8501:8600, 8601:8700, 8701:8800, 8801:8900, 8901:9000, 9001:9100, 9101:9200, 9201:9300, 9301:9400, 9401:9500, 9501:9600, 9601:9700, 9701:9800, 9801:9900, 9901:10000, 10001:10100, 10101:10200, 10201:10300, 10301:10400, 10401:10500, 10501:10600, 10601:10700, 10701:10800, 10801:10900, 10901:11000, 11001:11100, 11101:11200, 11201:11300, 11301:11400, 11401:11500, 11501:11600, 11601:11700, 11701:11800, 11801:11900, 11901:12000, 12001:12100, 12101:12200, 12201:12300, 12301:12400, 12401:12500, 12501:12600, 12601:12700, 12701:12800, 12801:12900, 12901:13000, 13001:13100, 13101:13200, 13201:13300, 13301:13400, 13401:13500, 13501:13600, 13601:13700, 13701:13800, 13801:13900, 13901:14000, 14001:14100, 14101:14200, 14201:14300, 14301:14400, 14401:14500, 14501:14600, 14601:14700, 14701:14800, 14801:14900, 14901:15000, 15001:15100, 15101:15200, 15201:15300, 15301:15400, 15401:15500, 15501:15600, 15601:15700, 15701:15800, 15801:15900, 15901:16000, 16001:16100, 16101:16200, 16201:16300, 16301:16400, 16401:16500, 16501:16600, 16601:16700, 16701:16800, 16801:16900, 16901:17000, 17001:17059)
if(pull_data)
{
	download_save_data(partitions = partitions, all_symbols = all_symbols, acwi_closing = acwi_closing)
	number_of_partitions <- length(partitions)
	financial_column_names <- readRDS(file = './Data/financial_column_names.RDS') # saveRDS(colnames(final_dataset), file = './data/financial_column_names.RDS')
	df_stocks_raw <- load_raw_stocks(number_of_partitions = number_of_partitions, financial_column_names = financial_column_names)

	saveRDS(df_stocks_raw, './data/df_stocks_raw.RDS')
}else
{
	df_stocks_raw <- readRDS('./data/df_stocks_raw.RDS')
}
```

## Check Data Integrity

```{r data_integrety, echo=FALSE}
df_temp <- df_stocks_raw %>% filter(symbol == 'A')

if(nrow(df_temp) < 3)
{
    stop('not enough A symbols')
}
if(!all(month(df_temp$date) == 10) || !all(day(df_temp$date) == 31))
{
    stop('incorrect dates')
}
if(!(((df_temp$stock_close_moving_avg[1] - df_temp$stock_close_moving_avg[2]) / df_temp$stock_close_moving_avg[2]) == df_temp$perc_change_stock_1year[2]))
{
    stop('incorrect perc_change_stock_1year (2)')
}
if(!(((df_temp$stock_close_moving_avg[2] - df_temp$stock_close_moving_avg[3]) / df_temp$stock_close_moving_avg[3]) == df_temp$perc_change_stock_1year[3]))
{
    stop('incorrect perc_change_stock_1year (3)')
}
```

## 'Cleaning' Stocks

```{r clean_add_ratios, echo=FALSE, message=FALSE, warning=FALSE, comment='', results='asis'}
df_stocks_full <- stock_financials_clean(df_stocks_raw)
df_stocks_full <- stock_financials_ratios(df_stocks_full)

cat(paste0('> `', length(unique(df_stocks_full$symbol)), '` unique stocks.\n\n'))
```

## Column Names:

```{r column_names, comment=NA}
column_names <- colnames(df_stocks_full)
kable(data_frame(num=(1:length(column_names)), colname_names=column_names))
```

## Creating Trend Dataset
```{r}
if(pull_data)
{
	df_stocks_trend <- build_stock_trend_dataset(df_stocks_full = df_stocks_full)

	saveRDS(df_stocks_trend, './data/df_stocks_trend.RDS')
}else
{
	df_stocks_trend <- readRDS('./data/df_stocks_trend.RDS')
}
```

## Data Summary:

> **NOTE**: all numbers, regardless of source (yahoo/google), look to be in `millions`. I have confirmed over multiple stocks/sources (but this can/should be further confirmed).

```{r summmary, comment=NA}
summary(df_stocks_full)
cat('\n\n')
summary(df_stocks_trend)
```

# Exploring Data

## Collinearity

```{r correlations}
kable(get_correlations(df_stocks_full, corr_threshold = 0.7, p_value_threshold = 0.2))
```

> The high correlations among absolute data (as opposed to ratios) suggests that using ratios might yield better or more valid results in models where multi-collinearity causes problems.
>
> Rather than removing all of these columns, I will keep them in for now (with the exception of Revenue which will be removed (TotalRevenue will remain)), and extract the columns I want to work with for particular models.

## Correlation, Maximal Information Coefficient

- The following graph shows correlations and the Maximal Information Coefficient (second column) between many of the potential predictor variables and `perc_change_stock_1year` (the change in stock price from the time the financial statement was released to 1 year later (using moving average to account for daily fluctuations and random noise).
    - it is faceted into 3 groups (`Absolute`, `Common Size`, and `Ratio`).
    - `Absolute` are raw numbers from the financial statements (e.g. Total Revenue)
    - `Common Size` are ratios expressed as a percentage of Sales, Assets, etc., depending on the particular financial statement (income statement, balance sheet, cash flow)
    - `Ratio` are common ratios used to access the health of the company (e.g. Profit Margin, Quick Ratio, etc.) 
- As you can see, the correlations are lower than expected, and a lot are negative (left of vertical reference line). Personally, I expected some of the financial ratios in particular to be higher correlated with the change of stock price (i.e. higher financial ratios mean healthier companies which means higher gains in stock price).
- As a result, I used the Maximal Information Coefficient (MIC) which, according to this [blog post](http://menugget.blogspot.de/2011/12/maximal-information-coefficient-mic.html) the MIC measurement is 'able to equally describe the correlation between paired variables regardless of linear or nonlinear relationship'.
- Interestingly, there is not much relationship between the correlation measure and MIC, and from looking ranking of both (not shown; only ranked by MIC in graph), it appears MIC is a better measure.
- Also, variables that are `absolute` values tend to have lower correlations (and more negative correlations) then `common size` and `ratio` variables, suggesting some/most absolute ratios should be ignored in linear regression models.

> Overall, the low/negative correlations suggests that my original plan of using a form of linear regression will probably not result in any significant results. 

```{r maximal_information_coefficient, echo=FALSE, fig.height=15, fig.width=10, message=FALSE, cached=TRUE}
if(pull_data)
{
	x <- df_stocks_full %>%
		select(-date, -symbol, -perc_change_stock_1year, -diff_above_index_1year, -dplyr::contains('ratioh_')) # get rid of `ratioh` because we only want numeric columns
	y <- df_stocks_full %>%
		select(diff_above_index_1year)

	correlations_with_perc_change <- cor(x, y=y, use='complete.obs')

	df_correlations <- data_frame(variable = rownames(correlations_with_perc_change), correlation = as.numeric(correlations_with_perc_change)) %>%
		mutate(correlation_rank = dense_rank(desc(abs(correlation)))) %>%
		arrange(correlation_rank) %>% select(variable, correlation, correlation_rank)

	# original implementation found http://menugget.blogspot.de/2014/09/maximal-information-coefficient-part-ii.html
	library(minerva)
	minerva_results <- mine(x, y = y$diff_above_index_1year, alpha = 0.7)
	res <- data.frame(variable = rownames(minerva_results$MIC), MIC = c(minerva_results$MIC))# %>%
	res <- res %>% mutate(MIC_rank = dense_rank(desc(MIC))) %>% arrange(MIC_rank)
	final_correlations = inner_join(res, df_correlations, by = 'variable')

	saveRDS(final_correlations, './data/maximal_information_coefficient_correlations.RDS')
}else
{
	final_correlations <- readRDS('./data/maximal_information_coefficient_correlations.RDS')
}

long_correaltions <- final_correlations %>%
	mutate(variable = factor(variable, levels = rev(variable))) %>%
	select(variable, MIC, correlation) %>%
	gather(metric, amount, -variable)

long_correaltions$type <- 'absolute'
long_correaltions$type <- ifelse(grepl('^cs_', long_correaltions$variable), 'common size', long_correaltions$type)
long_correaltions$type <- ifelse(grepl('^ratios_', long_correaltions$variable), 'ratio', long_correaltions$type)

ggplot(long_correaltions, aes(x = amount, y = variable, col = metric)) +
	geom_point(size = 2, alpha = 0.9) +
	geom_ref_line(v = 0) +
    facet_grid(type ~ ., space = 'free_y', scales = "free") +
	ggtitle('MIC & Correlations') + ylab('Variables') + xlab('Amount') + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + guides(fill=FALSE)
```

```{r, echo = FALSE}
df_top_variables <- final_correlations %>% 
    filter(MIC_rank <= 10 | correlation_rank <= 10)
```

## Looking at Data 

- Looking at variables against `diff_above_index_1year`

### Variables with High MIC

```{r mic_graphs, echo=FALSE}
walk(df_top_variables$variable, ~{
	column_name <- .
	temp_data <- data.frame(x_variable = df_stocks_full[, column_name], diff_above_index_1year = df_stocks_full$diff_above_index_1year)

	x_zoom_max <- quantile(temp_data$x_variable,  probs = c(0.90))
	x_zoom_min <- quantile(temp_data$x_variable,  probs = c(0.10))

	print(ggplot(data = temp_data, mapping = aes(x = x_variable, y = diff_above_index_1year)) +
		geom_point(alpha=0.2) + 
		coord_cartesian(ylim = c(-1, 2), xlim = c(x_zoom_min, x_zoom_max)) +
		xlab(column_name))
})
```

> No clear linear patterns found (visually), which is interesting.

### Trend Variables

```{r trend_graphs, echo=FALSE}
column_names <- colnames(df_stocks_trend %>% select(-date, -symbol))

walk(column_names, ~{
	column_name <- .
	temp_data <- data.frame(x_variable = df_stocks_trend[, column_name], diff_above_index_1year = df_stocks_trend$diff_above_index_1year)

	x_zoom_max <- quantile(temp_data$x_variable,  probs = c(0.90))
	x_zoom_min <- quantile(temp_data$x_variable,  probs = c(0.10))

	print(ggplot(data = temp_data, mapping = aes(x = x_variable, y = diff_above_index_1year)) +
		geom_point(alpha=0.2) + 
		coord_cartesian(ylim = c(-1, 2), xlim = c(x_zoom_min, x_zoom_max)) +
		xlab(column_name))
})
```

> No clear linear patterns found (visually), which is interesting.

# Clustering

- Before we get into prediction, I'm wondering if there distinguishable segments that we could organic the stocks into, from which patterns (in the dependent variables) would would arise.
- I'm going to try to leave the dependent variables out (e.g. `diff_above_index_1year`) and then add them back in after clusting (because presumably if we would like to use clustering to aid in prediction, we would have to cluster on only the independent variables that we would have at the time of clustering/prediction).

> I will be using hierarchical clustering, primarily because k-means clustering starts with a random choice of cluster center and, therefore, will most likely yield different results each time it is ran (with the same data). Hierarchical will most likely be more consistent.

```{r segmentation, echo=FALSE}
df_seg_common_size <- df_stocks_full %>% select(perc_change_stock_1year, diff_above_index_1year, dplyr::contains('cs_'))
df_seg_top_variables <- df_stocks_full[ , c('perc_change_stock_1year', 'diff_above_index_1year', df_top_variables$variable)]
df_seg_trend <- df_stocks_trend %>% dplyr::select(-date, -symbol)

segmentation_list <- list(df_seg_common_size, df_seg_top_variables, df_seg_trend)
segmentation_RDSs <- list('common size', 'top variables', 'trend')

walk2(segmentation_list, segmentation_RDSs, ~ {

	segmentation_name <- .y
	segmentation_file_name <- str_replace(segmentation_name, ' ', '_')
	ideal_number_of_clusters_file <- paste0('./data/segmentation_ideal_number_of_clusters_', segmentation_file_name,'.RDS')
	clusters_file_name <- paste0('./data/segmentation_clusters_', segmentation_file_name,'.RDS')

	cat(paste('\n\n## Clustering -', segmentation_name, '\n\n'))

	segmentation_dataset <- .x
	dataset_scaled <- scale(segmentation_dataset)

	if(pull_data)
	{
		ideal_number_of_clusters <- pamk(dataset_scaled)$nc
		distances <- dist(dataset_scaled, method = "euclidean")
		clusters <- hclust(distances, method = "ward.D")

		saveRDS(ideal_number_of_clusters, file = ideal_number_of_clusters_file)
		saveRDS(clusters, file = clusters_file_name)
	}else
	{
		ideal_number_of_clusters <- readRDS(ideal_number_of_clusters_file)
		clusters <- readRDS(clusters_file_name)
	}

	cat(paste('> ideal number of clusters:', ideal_number_of_clusters, '\n\n'))

	all_cluster_groups <- map(2:5, ~ 
	{
		number_of_clusters <- .
		clusterGroups <- cutree(clusters, k = number_of_clusters) # assigns a cluster group to each index
		cluster_splits <- split(as.data.frame(dataset_scaled), clusterGroups) # this adds back in is_stage_9_paying

		results_df <- lapply(cluster_splits, colMeans) # command will output the cluster centroids for all clusters:
		results_df <- do.call(cbind, lapply(results_df, data.frame, stringsAsFactors=FALSE))

		results_df <- as.data.frame(t(results_df)) # t (transpose) results in a matrix, convert back to df
		results_df$cluster_name <- 1:nrow(results_df)
		
		print(cluster_heatmap(results_df = results_df))

		cluster_summary <- segmentation_dataset %>%
			mutate(cluster = clusterGroups) %>%
			dplyr::group_by(cluster) %>%
			dplyr::summarise(n = n(),
				perc_change_mean = mean(perc_change_stock_1year),
				perc_change_sd = sd(perc_change_stock_1year),
				diff_above_index_mean = mean(diff_above_index_1year),
				diff_above_index_sd = sd(diff_above_index_1year))

		print(kable(cluster_summary))

		return(clusterGroups)
	})
})
```

> It will be an interesting experiment to divide the stocks up by cluster to see if machine learning algorithms predict better when focusing on clustered data. The drawback will be a reduced test/training set.

---------------------------------------------------------------------------------------------------------------------------------------

# Sandbox / TODO

```{r sandbox}
ggplot(data = df_stocks_full, mapping = aes(x = DividendsperShareCommonStockPrimaryIssue, y = diff_above_index_1year)) +
	geom_point(alpha=0.2)

ggplot(data = df_stocks_full, mapping = aes(x = ratios_quick_ratio, y = diff_above_index_1year)) +
	geom_point(alpha=0.2)

ggplot(data = df_stocks_full, mapping = aes(x = TotalRevenue, y = diff_above_index_1year)) +
	geom_point(alpha=0.2)

# makes sense that revenue isn't necessarily correlated with diff_above_index_1year becuase revenue doesn't guarantee success or profit

ggplot(data = df_stocks_full) +
	geom_hex(mapping = aes(x = TotalRevenue, y = diff_above_index_1year))

ggplot(data = df_stocks_full, aes(diff_above_index_1year)) +
	geom_freqpoly(bins = 15)

# perhaps clustering data will help, since 

ggplot(data = df_stocks_full, mapping = aes(x = net_profit_margin, y = diff_above_index_1year)) +
	geom_point(alpha=0.2)

# makes sense that revenue isn't necessarily correlated with diff_above_index_1year becuase revenue doesn't guarantee success or profit

ggplot(data = df_stocks_full) +
	geom_hex(mapping = aes(x = net_profit_margin, y = diff_above_index_1year))

ggplot(data = df_stocks_full, aes(net_profit_margin)) +
	geom_freqpoly(bins = 15)

# interesting that net profit margin doesn't appear to have any significance in determining diff_above_index_1year. I would have expected a somewhat linear relationship
```

# Datasets

```{r}
stocks_common_size_perc_change <- df_stocks_full %>% dplyr::select(perc_change_stock_1year, dplyr::contains('cs_'))
saveRDS(stocks_common_size_perc_change, file = 'stocks_common_size_perc_change.RDS')
stocks_common_size_diff_above_index <- df_stocks_full %>% dplyr::select(diff_above_index_1year, dplyr::contains('cs_'))
saveRDS(stocks_common_size_diff_above_index, file = 'stocks_common_size_diff_above_index.RDS')

stocks_ratios_perc_change <- df_stocks_full %>% dplyr::select(perc_change_stock_1year, dplyr::contains('ratios_'), dplyr::contains('ratioh_'))
saveRDS(stocks_ratios_perc_change, file = 'stocks_ratios_perc_change.RDS')
stocks_ratios_diff_above_index <- df_stocks_full %>% dplyr::select(diff_above_index_1year, dplyr::contains('ratios_'), dplyr::contains('ratioh_'))
saveRDS(stocks_ratios_diff_above_index, file = 'stocks_ratios_diff_above_index.RDS')

stocks_top_variables_perc_change <- df_stocks_full[, c('perc_change_stock_1year', df_top_variables$variables)]
saveRDS(stocks_top_variables_perc_change, file = 'stocks_top_variables_perc_change.RDS')
stocks_top_variables_diff_above_index <- df_stocks_full[, c('diff_above_index_1year', df_top_variables$variables)]
saveRDS(stocks_top_variables_diff_above_index, file = 'stocks_top_variables_diff_above_index.RDS')

stocks_all_perc_change <- df_stocks_full %>% dplyr::select(-date, -symbol, -diff_above_index_1year)
saveRDS(stocks_all_perc_change, file = 'stocks_all_perc_change.RDS')
stocks_all_diff_above_index <- df_stocks_full %>% dplyr::select(-date, -symbol, -perc_change_stock_1year)
saveRDS(stocks_all_diff_above_index, file = 'stocks_all_diff_above_index.RDS')

stocks_trend_perc_change <- df_stocks_trend %>% dplyr::select(-date, -symbol, -diff_above_index_1year)
saveRDS(stocks_trend_perc_change, file = 'stocks_trend_perc_change.RDS')
stocks_trend_diff_above_index <- df_stocks_trend %>% dplyr::select(-date, -symbol, -perc_change_stock_1year)
saveRDS(stocks_trend_diff_above_index, file = 'stocks_trend_diff_above_index.RDS')
```
